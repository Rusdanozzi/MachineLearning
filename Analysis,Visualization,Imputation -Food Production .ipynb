{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Analysis, Visualization and Extensive Imputation of CO2 Emissions in Food Products"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-one\"></a>\n","# 1. Introduction"]},{"cell_type":"markdown","metadata":{},"source":["**Objectives:** The objectives of this analysis are two-fold. First, to provide an investigation of the dataset within the archetype of a scientist and researcher attempting to compress information to be used for colleagues at various positions (e.g. engineers might require more operational insight, while management will require an overview). Second, to provide a walkthrough for all fellow coders and statisticians through descriptive actions.\n","\n","**Descriptive actions:** I tried to describe the reasoning behind (almost) every line of code during the examination of this dataset - especially during the feature engineering, imputation, and analysis sections. This extra work serves two functions: first to help new learners understand all featured techniques and second, to allow more experienced statisticians and coders to correct my code or way of approaching the problem - even through there are some serious limitations about this dataset.\n","\n","**Limitations:** It is also important to note that since we have <50 entries there is a very high probability that many models are inadvisable to use since we would not be able to obtain a sufficient alpha level. This includes many estimator models we utilize with sklearn but also with any correlation, regression and other analysis performed. More on this later on. Finally, I tried to adhere to the Zen of Python rules, however, some have been deliberately broken in order to showcase certain analyses."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-two\"></a>\n","# 2. Importing Libraries and Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.529702Z","iopub.status.busy":"2023-01-19T00:07:47.529275Z","iopub.status.idle":"2023-01-19T00:07:47.537918Z","shell.execute_reply":"2023-01-19T00:07:47.536892Z","shell.execute_reply.started":"2023-01-19T00:07:47.529669Z"},"trusted":true},"outputs":[],"source":["import math as math\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import squarify as sqf\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.linear_model import BayesianRidge"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.545505Z","iopub.status.busy":"2023-01-19T00:07:47.544988Z","iopub.status.idle":"2023-01-19T00:07:47.560582Z","shell.execute_reply":"2023-01-19T00:07:47.55926Z","shell.execute_reply.started":"2023-01-19T00:07:47.545467Z"},"trusted":true},"outputs":[],"source":["df_food = pd.read_csv(\"../input/environment-impact-of-food-production/Food_Production.csv\")\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.563709Z","iopub.status.busy":"2023-01-19T00:07:47.562703Z","iopub.status.idle":"2023-01-19T00:07:47.569718Z","shell.execute_reply":"2023-01-19T00:07:47.568325Z","shell.execute_reply.started":"2023-01-19T00:07:47.563644Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-three\"></a>\n","# 3. Overview"]},{"cell_type":"markdown","metadata":{},"source":["First, let us start in the standard way and have a quick overview of our dataset prior to feature engineering and subsequent analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.572834Z","iopub.status.busy":"2023-01-19T00:07:47.572325Z","iopub.status.idle":"2023-01-19T00:07:47.585485Z","shell.execute_reply":"2023-01-19T00:07:47.584487Z","shell.execute_reply.started":"2023-01-19T00:07:47.572789Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["df_food.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.588528Z","iopub.status.busy":"2023-01-19T00:07:47.587775Z","iopub.status.idle":"2023-01-19T00:07:47.621337Z","shell.execute_reply":"2023-01-19T00:07:47.619921Z","shell.execute_reply.started":"2023-01-19T00:07:47.588489Z"},"trusted":true},"outputs":[],"source":["df_food.head(3)\n","\n","# A few first-sight findings: \n","# We notice long-winded colunm names, naming inconsistencies, overwhelming usage of numeric columns, descriptive statistics will most likely revolve around the food products and certain key features/columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.624889Z","iopub.status.busy":"2023-01-19T00:07:47.624082Z","iopub.status.idle":"2023-01-19T00:07:47.704918Z","shell.execute_reply":"2023-01-19T00:07:47.70374Z","shell.execute_reply.started":"2023-01-19T00:07:47.624853Z"},"trusted":true},"outputs":[],"source":["df_food.describe(include='all')\n","\n","# Negative values can be spotted under the [Land Use Change] column (notice the minumum values)\n","# Incosistencies detected in the 75% percentile range for certain columns (e.g. [Animal Feed] and [Transport])\n","# Radical differences in min/max in latter columns (e.g. [Scarcity Water])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.708058Z","iopub.status.busy":"2023-01-19T00:07:47.707664Z","iopub.status.idle":"2023-01-19T00:07:47.72376Z","shell.execute_reply":"2023-01-19T00:07:47.722333Z","shell.execute_reply.started":"2023-01-19T00:07:47.708011Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["df_food.info()\n","\n","# Mostly numeric variables. Several missing values which already point towards a Missing Not At Random (MNAR) archetype.\n","# We shall deal with them in the Feature Engineering Section."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-four\"></a>\n","# 4. Feature Engineering"]},{"cell_type":"markdown","metadata":{},"source":["We have a few issues to address before we can proceed, we need to:\n","- First, rename our columns\n","- Second, scout for any negative values\n","- Third, deal with any missing values\n","- Fourth, assuming that we have taken the task to assist science, engineering, or business operations, we might need to make certain data transformations in order to assist our colleagues who will surely have varying needs (e.g. generalized vs specific, product-focused vs process-focused, etc.)."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-four\"></a>\n","## 4.1 Renaming Columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.726537Z","iopub.status.busy":"2023-01-19T00:07:47.726134Z","iopub.status.idle":"2023-01-19T00:07:47.744372Z","shell.execute_reply":"2023-01-19T00:07:47.742924Z","shell.execute_reply.started":"2023-01-19T00:07:47.726483Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# The column names in the dataset are quite inconsistent (some use underscores, others spaces, there are letters missing, etc.) and some columns have long names that need to be revisited.\n","\n","# Creating short dataframes containing a keyword from the various columns\n","df_eutro = df_food.loc[:, [\"Eutro\" in i for i in df_food.columns]]\n","df_freshwater = df_food.loc[:, [\"Freshwater\" in i for i in df_food.columns]]\n","df_gas = df_food.loc[:, [\"gas\" in i for i in df_food.columns]]\n","df_land = df_food.loc[:, [\"Land\" in i for i in df_food.columns]]\n","df_scarc_water = df_food.loc[:, [\"Scarcity\" in i for i in df_food.columns]]\n","\n","# Correcting the inconsistent naming patterns and renaming the long-winded columns\n","df_food.rename(columns = {\n","    'Food product' : 'Food_Product',\n","    'Packging' : 'Packaging',\n","    'Total_emissions' : 'Total_Emissions',\n","    'Animal Feed' : 'Animal_Feed',\n","    df_eutro.columns[0] : \"Eutro_Em_1000kcal\",\n","    df_eutro.columns[1] : \"Eutro_Em_1kg\",\n","    df_eutro.columns[2] : \"Eutro_Em_100gProtein\",\n","    df_freshwater.columns[0] : \"Freshwater_1000kcal\",\n","    df_freshwater.columns[1] : \"Freshwater_100gProtein\",\n","    df_freshwater.columns[2] : \"Freshwater_1kg\",\n","    df_gas.columns[0] : \"GGas_Em_1000kcal\",\n","    df_gas.columns[1] : \"GGas_Em_100gProtein\",\n","    df_land.columns[0] : \"Land_Use_Change\",\n","    df_land.columns[1] : \"Land_Use_1000kcal\",\n","    df_land.columns[2] : \"Land_Use_1kg\",\n","    df_land.columns[3] : \"Land_Use_100gProtein\",\n","    df_scarc_water.columns[0] : \"ScarcWater_1kg\",\n","    df_scarc_water.columns[1] : \"ScarcWater_100gProtein\",\n","    df_scarc_water.columns[2] : \"ScarcWater_1000kcal\"\n","}, inplace=True)\n","\n","print(\"New column names: \\n\")\n","for col in df_food:\n","    print(col)"]},{"cell_type":"markdown","metadata":{},"source":["Low number of unique values?\n","\n","On a secondary nature, it would be a fair assumption to expect different food products to have different CO2 emission measurements for certain features/columns like [Retail], [Transport], [Packaging], and [Animal Feed]. However, they have a surprising low amount of unique values, as can be seen in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.747928Z","iopub.status.busy":"2023-01-19T00:07:47.746358Z","iopub.status.idle":"2023-01-19T00:07:47.765929Z","shell.execute_reply":"2023-01-19T00:07:47.76455Z","shell.execute_reply.started":"2023-01-19T00:07:47.747888Z"},"trusted":true},"outputs":[],"source":["print(\"Unique values - standardized agriculture operations: \" + \"\\n\\n\"\n","    + str(df_food[[\"Retail\", \"Transport\", \"Packaging\", \"Animal_Feed\"]].nunique()) + \"\\n\\n\"\n","    + \"Top 10 Retail Values for Food Products\" + \"\\n\"\n","    + str(df_food[['Food_Product', 'Retail', 'Transport']].sort_values(by='Retail', ascending=False).head(10)))"]},{"cell_type":"markdown","metadata":{},"source":["Out of 42 entries in our dataset, [Retail] has only four unique values,[Transport] has eight, and [Packaging] and [Animal_Feed] have ten.\n","\n","This can be justified by considering the following:\n","1. Primary hypothesis: these features deal with industrialized agricultural techniques, therefore, it is expected that many package-transport-retail processes are standardized.\n","2. Secondary and less likely hypothesis: these entries may have been assigned values as part of a clustering method rather than on an individual basis.\n","3. Note that [\"Animal_Feed\"] concerns only food products with animal food products, therefore it applies only to those 10 entries."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-four\"></a>\n","## 4.2 Addressing Negative Values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.769196Z","iopub.status.busy":"2023-01-19T00:07:47.768507Z","iopub.status.idle":"2023-01-19T00:07:47.788094Z","shell.execute_reply":"2023-01-19T00:07:47.786481Z","shell.execute_reply.started":"2023-01-19T00:07:47.769147Z"},"trusted":true},"outputs":[],"source":["# First, let us scout for negative values in our dataset and, if detected, determine how to proceed with the analysis.\n","\n","df_numeric = df_food.select_dtypes('number')\n","neg_values = (df_numeric<0).sum().sum()\n","\n","print(f\"Number of negative values in the dataset: {neg_values}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.790306Z","iopub.status.busy":"2023-01-19T00:07:47.789668Z","iopub.status.idle":"2023-01-19T00:07:47.805446Z","shell.execute_reply":"2023-01-19T00:07:47.80412Z","shell.execute_reply.started":"2023-01-19T00:07:47.790269Z"},"trusted":true},"outputs":[],"source":["# It would appear that only only column holds negative values: Land-Use Change\n","\n","(df_numeric<0).sum().sort_values(ascending=False).head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.809433Z","iopub.status.busy":"2023-01-19T00:07:47.808997Z","iopub.status.idle":"2023-01-19T00:07:47.826095Z","shell.execute_reply":"2023-01-19T00:07:47.824856Z","shell.execute_reply.started":"2023-01-19T00:07:47.8094Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# There are four entries of food products in Land-Use Change that hold negative values.\n","\n","df_food[df_food['Land_Use_Change']<0][['Food_Product', 'Land_Use_Change']]"]},{"cell_type":"markdown","metadata":{},"source":["As an example of a more deep-dive approach to understanding our dataset let's focus on the most prevalent negative value: why does the \"Nuts\" entry have a -2.1 value?\n","\n","As is noted in the website OurWorldInData (https://ourworldindata.org/faqs-environmental-impacts-food):\n","\"In the recent past, many nut plantations have been replaced grasslands or abandoned pastures. Since the trees of nut crops sequester carbon dioxide, when they replace some grasslands this can actually result in an emission saving due to positive land use change. This effect, however, will eventually diminish as nut plantations are grown on land that was not previously grasslands.\""]},{"cell_type":"markdown","metadata":{},"source":["Negative values will hinder our ability to properly visualize the data.\n","\n","Therefore we shall proceed with the following choices:\n","1. Made a note of our discovery and provided scientific evidence regarding its presence.\n","2. Replace all negative values with zero so we can proceed with the analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.828972Z","iopub.status.busy":"2023-01-19T00:07:47.82841Z","iopub.status.idle":"2023-01-19T00:07:47.842166Z","shell.execute_reply":"2023-01-19T00:07:47.840277Z","shell.execute_reply.started":"2023-01-19T00:07:47.828926Z"},"trusted":true},"outputs":[],"source":["# We can iterate through the columns in our dataframe that hold numeric values in order to detect and replace all negative values with zero\n","\n","for col in df_food.iloc[:, df_food.columns.get_loc('Land_Use_Change'):df_food.columns.get_loc('ScarcWater_1000kcal')]:\n","    for ind, entry in enumerate(df_food[col]):\n","        if entry < 0:\n","            df_food.at[ind, col] = 0\n","print(f\"Number of negative values in the dataset: {((df_food.iloc[:,1:-1])<0).sum().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.844674Z","iopub.status.busy":"2023-01-19T00:07:47.844312Z","iopub.status.idle":"2023-01-19T00:07:47.880231Z","shell.execute_reply":"2023-01-19T00:07:47.879204Z","shell.execute_reply.started":"2023-01-19T00:07:47.844642Z"},"trusted":true},"outputs":[],"source":["# Creating a food category column which we will retain for all following sections of this analysis.\n","\n","df_food[\"Category\"] = df_food[\"Food_Product\"] # creating a new column with the exact list of [Food_Products]\n","\n","# Setting various lists for different types of [Food_Products]\n","Grains = [\"Wheat & Rye (Bread)\", \"Maize (Meal)\", \"Oatmeal\", \"Barley (Beer)\", \"Rice\"]\n","Nuts = ['Nuts', 'Groundnuts']\n","Vegetables = [\"Potatoes\", \"Cassava\", 'Other Pulses',\"Peas\",'Tomatoes', 'Onions & Leeks','Root Vegetables',\"Brassicas\",'Other Vegetables']\n","Fruits = ['Citrus Fruit', 'Bananas','Apples', 'Berries & Grapes', 'Other Fruit']\n","Sugars = ['Cane Sugar', 'Beet Sugar',]\n","Oils = ['Soybean Oil', 'Palm Oil', 'Sunflower Oil', 'Rapeseed Oil', 'Olive Oil']\n","Dairy = [\"Soymilk\",'Milk', 'Cheese']\n","Animal_Prod = ['Beef (beef herd)', 'Beef (dairy herd)','Lamb & Mutton', 'Pig Meat', 'Poultry Meat', 'Eggs', 'Fish (farmed)', 'Shrimps (farmed)']\n","Other = [\"Tofu\", \"Coffee\", \"Dark Chocolate\", \"Wine\"]\n","\n","# Replacing all [Food_Products] in the newly developed column with their respective food [Category]\n","for i in df_food[\"Category\"]:\n","    if i in Grains:\n","        df_food[\"Category\"].replace([i], \"Grains\", inplace=True)\n","    elif i in Nuts:\n","        df_food[\"Category\"].replace([i], \"Nuts\", inplace=True)\n","    elif i in Vegetables:\n","        df_food[\"Category\"].replace([i], \"Vegetables\", inplace=True)\n","    elif i in Fruits:\n","        df_food[\"Category\"].replace([i], \"Fruits\", inplace=True)\n","    elif i in Sugars:\n","        df_food[\"Category\"].replace([i], \"Sugar\", inplace=True)\n","    elif i in Oils:\n","        df_food[\"Category\"].replace([i], \"Oils\", inplace=True)\n","    elif i in Dairy:\n","        df_food[\"Category\"].replace([i], \"Dairy\", inplace=True)\n","    elif i in Animal_Prod:\n","        df_food[\"Category\"].replace([i], \"Animal_Prod\", inplace=True)\n","    elif i in Other:\n","        df_food[\"Category\"].replace([i], \"Other\", inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-four\"></a>\n","## 4.3.1 Overview of Missing Values"]},{"cell_type":"markdown","metadata":{},"source":["In the previous overview section we noticed that there are quite a few missing values.\n","\n","To have a better overview we can create an informative table that will show us the following stats:\n","1. which columns have missing values\n","2. their data types\n","3. the prevalence of missing values based on columns\n","4. the percentage of missing values based on total length of observations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.882563Z","iopub.status.busy":"2023-01-19T00:07:47.882102Z","iopub.status.idle":"2023-01-19T00:07:47.910616Z","shell.execute_reply":"2023-01-19T00:07:47.909401Z","shell.execute_reply.started":"2023-01-19T00:07:47.882518Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["n_NAvalues = df_food.isna().sum()\n","perc_NAvalues = round(df_food.isna().sum()/len(df_food)*100,ndigits=1)\n","\n","table_NA_info = pd.DataFrame({\n","    \"Data Types\": df_food.dtypes,\n","    \"Unique Values\" : df_food.nunique(),\n","    \"Total NA Values\": n_NAvalues,\n","    \"%Perc NA Values\": perc_NAvalues})\n","\n","table_NA_info.sort_values(by = \"%Perc NA Values\", ascending = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.913326Z","iopub.status.busy":"2023-01-19T00:07:47.912567Z","iopub.status.idle":"2023-01-19T00:07:47.927221Z","shell.execute_reply":"2023-01-19T00:07:47.926355Z","shell.execute_reply.started":"2023-01-19T00:07:47.913279Z"},"trusted":true},"outputs":[],"source":["# As a top-level summary we can explicitly show a key few stats about missing values.\n","# Approximately 33% of all columns in our dataset have missing values.\n","# At the highest missing value threshold we have 7 columns with 30% and above missing values,\n","# 3 columns with 20% to 30% missing values, and 10 columns with 10% to 20% missing values.\n","\n","upper_NA_threshold = len(table_NA_info[table_NA_info[\"%Perc NA Values\"]>30])\n","middle_NA_threshold = len(table_NA_info[(table_NA_info[\"%Perc NA Values\"]>20) & (table_NA_info[\"%Perc NA Values\"]<30)])\n","low_NA_threshold = len(table_NA_info[(table_NA_info[\"%Perc NA Values\"]>10) & (table_NA_info[\"%Perc NA Values\"]>20)])\n","perc_NA = round((sum(df_food.isna().any())/len(df_food))*100,ndigits=2)\n","\n","print(\"Percentage of columns with NA values in original dataset: \" + \"%\" + str(perc_NA) + \"\\n\"\n","      \"Number of columns with 30% to 40% missing values: \" + str(upper_NA_threshold) + \"\\n\"\n","      \"Number of columns with 20% to 30% missing values: \" + str(middle_NA_threshold) + \"\\n\"\n","      \"Number of columns with 10% to 20% missing values: \" + str(low_NA_threshold))"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"subsection-four-three-two\"></a>\n","### 4.3.2 MNAR Archetype and systematic bias for missing values\n","\n","How can we interpret the information we already gathered?\n","\n","We notice that the majority of missing values in the table_NA_info above are primary found on all columns that deal with protein measurements (37%-40% NA values), followed by all variables dealing with kcal measurements (20% to 30% NA values), and finally by kg measurements (11.6% NA values).\n","\n","As such, this points towards the \"Missing Not At Random\" (MNAR) archetype, meaning there is a systematic bias in the missing pattern (i.e. it is not completely random) since the missing values are found in specific columns rather than randomly in our dataset."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-four\"></a>\n","### 4.3.3 Imputation Methods"]},{"cell_type":"markdown","metadata":{},"source":["We need to choose an imputation method. Among others, our choices include the following methods:\n","1. Dropping columns with missing values\n","2. Imputation by average value \n","3. Imputation by median\n","4. Imputation by K nearest neighbors (KNN)\n","5. Multiple imputation by chained equations (MICE)\n","6. Hot-deck A: custom-fitted imputation using average value\n","7. Hot-deck B: custom-fitted imputation using MICE\n","\n","In the next section we will explode these imputation methods in different temporary datasets prior to making final choice.\n","\n","Under each scenario we will choose three columns with the highest amount of missing values (as shown in the [table_NA_info] above) as examples to illustrate a few highlights of the results of each method and make a brief, top-level note of the differences of the mean values of each of these column after every imputation method."]},{"cell_type":"markdown","metadata":{},"source":["#### 1) Dropping columns with missing values"]},{"cell_type":"markdown","metadata":{},"source":["As shown in section 4.3.1, approximately 32% of our columns have missing data. We could go with the easy route and simply drop all columns with NA values but to waste such valuable information in such a small dataset with few entries (we only have 43 entries of food products!) is not expected to be particularly helpful."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.928953Z","iopub.status.busy":"2023-01-19T00:07:47.92843Z","iopub.status.idle":"2023-01-19T00:07:47.947948Z","shell.execute_reply":"2023-01-19T00:07:47.947025Z","shell.execute_reply.started":"2023-01-19T00:07:47.928919Z"},"trusted":true},"outputs":[],"source":["# Dropping NA values would be easy thing to do, however, it might not be the most scientific approach.\n","\n","diff = round((len(df_food.dropna())/len(df_food))*100, ndigits=2)\n","             \n","print(\"Number of rows in original dataset: \" + str(len(df_food)) + \"\\n\"\n","      \"Number of rows after dropping NA values: \" + str(len(df_food.dropna())) + \"\\n\"\n","      \"Percentage difference: \" + \"%\" + str(diff))\n","\n","# Indeed, if we choose to drop NA values we would lose a staggesting 55.8% of the observations (rows) in our original dataset."]},{"cell_type":"markdown","metadata":{},"source":["\n","Performing a drop_na function will cause numerous problems including, among others:\n","1. cause a significant loss of statistical power (even in a dataset with <50 entries)\n","2. lower the representation of various food products across key features\n","3. affect the correlations between said features\n","4. lower the dependability of our regression analyses"]},{"cell_type":"markdown","metadata":{},"source":["#### 2) Imputation by average value (mean)"]},{"cell_type":"markdown","metadata":{},"source":["Imputation using mean or median are two univariate approaches that are particularly popular due to their ease of implementation and in our case are definetely one step above the previous method of simply dropping all columns with missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.949932Z","iopub.status.busy":"2023-01-19T00:07:47.949406Z","iopub.status.idle":"2023-01-19T00:07:47.967084Z","shell.execute_reply":"2023-01-19T00:07:47.966161Z","shell.execute_reply.started":"2023-01-19T00:07:47.949888Z"},"trusted":true},"outputs":[],"source":["# Mean Imputation\n","\n","df_mean = df_food.copy(deep=True)\n","\n","df_mean = df_mean.fillna(df_mean.mean())\n","df_mean_na_sum = df_mean.isna().sum().sum()\n","\n","print(f\"Method: Mean Imputation \\n\" \n","      + f\"Number of missing values in dataset: {df_mean_na_sum} \\n\\n\"\n","      + f\"Mean value in Eutro_Em_1000kcal: {round(df_mean['Eutro_Em_1000kcal'].mean(),2)} \\n\"\n","      + f\"Mean value in Freshwater_100gProtein: {round(df_mean['Freshwater_100gProtein'].mean(),2)} \\n\"\n","      + f\"Mean value in Eutro_Em_100gProtein: {round(df_mean['Eutro_Em_100gProtein'].mean(),2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:47.968974Z","iopub.status.busy":"2023-01-19T00:07:47.968428Z","iopub.status.idle":"2023-01-19T00:07:48.05746Z","shell.execute_reply":"2023-01-19T00:07:48.056236Z","shell.execute_reply.started":"2023-01-19T00:07:47.968943Z"},"trusted":true},"outputs":[],"source":["df_mean.describe()"]},{"cell_type":"markdown","metadata":{},"source":["#### 3) Imputation by median value"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.059804Z","iopub.status.busy":"2023-01-19T00:07:48.059312Z","iopub.status.idle":"2023-01-19T00:07:48.080906Z","shell.execute_reply":"2023-01-19T00:07:48.079416Z","shell.execute_reply.started":"2023-01-19T00:07:48.059759Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Median Imputation: the process is quite similar to imputation by mean\n","\n","df_median = df_food.copy(deep=True)\n","\n","df_median = df_median.fillna(df_median.median())\n","df_median_na_sum = df_median.isna().sum().sum()\n","\n","print(f\"Method: Median Imputation \\n\" \n","      + f\"Number of missing values in dataset: {df_median_na_sum} \\n\\n\"\n","      \n","      + f\"Median value in Eutro_Em_1000kcal: {(round(df_median['Eutro_Em_1000kcal'].median(),ndigits=1))} \\n\"\n","      + f\"Mean value in Eutro_Em_1000kcal: {round(df_median['Eutro_Em_1000kcal'].mean(),2)} \\n\\n\"\n","      \n","      + f\"Median value in Freshwater_100gProtein: {round(df_median['Freshwater_100gProtein'].median(),2)} \\n\"\n","      + f\"Mean value in Freshwater_100gProtein: {round(df_median['Freshwater_100gProtein'].mean(),2)} \\n\\n\"\n","      \n","      + f\"Median value in Eutro_Em_100gProtein: {round(df_median['Eutro_Em_100gProtein'].median(),2)}\\n\"\n","      + f\"Mean value in Eutro_Em_100gProtein: {round(df_median['Eutro_Em_100gProtein'].mean(),2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.083313Z","iopub.status.busy":"2023-01-19T00:07:48.082891Z","iopub.status.idle":"2023-01-19T00:07:48.167743Z","shell.execute_reply":"2023-01-19T00:07:48.166399Z","shell.execute_reply.started":"2023-01-19T00:07:48.08328Z"},"trusted":true},"outputs":[],"source":["df_median.describe()"]},{"cell_type":"markdown","metadata":{},"source":["#### 4) KNN Imputation"]},{"cell_type":"markdown","metadata":{},"source":["Clustering-based imputation is a very useful tool. This is particularly true in datasets like this where the entries of certain variables are naturally grouped together which, in turn, makes predicting of missing values a lot easier."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.174027Z","iopub.status.busy":"2023-01-19T00:07:48.172857Z","iopub.status.idle":"2023-01-19T00:07:48.20394Z","shell.execute_reply":"2023-01-19T00:07:48.202364Z","shell.execute_reply.started":"2023-01-19T00:07:48.17398Z"},"trusted":true},"outputs":[],"source":["df_numeric = df_food.iloc[:,1:-1] # include only numeric columns \n","\n","from sklearn.impute import KNNImputer\n","knn_imputer = KNNImputer(missing_values = np.nan, n_neighbors = 5, weights = \"distance\", metric = \"nan_euclidean\")\n","# the reason we opt for weighted distance is to alleviate at least some of the issues from outliers (e.g. from Food_Product emission values like Beef)\n","# also, opting for hyperparameter n = 5, hypothesizing that there will be approximately five different overaching food categories for all food products (see the Hot Deck custom approach section for more info)\n","df_knn = pd.DataFrame(knn_imputer.fit_transform(df_numeric), columns = df_numeric.columns)\n","\n","df_knn.insert(0, \"Food_Product\", df_food[\"Food_Product\"]) # re-attaching the excluded [Food_Product] column\n","df_knn_na_sum = df_knn.isna().sum().sum()\n","\n","print(f\"Method: KNN Imputation \\n\" \n","      + f\"Number of missing values in dataset: {df_knn_na_sum} \\n\\n\"\n","      + f\"Mean value in Eutro_Em_1000kcal: {round(df_knn['Eutro_Em_1000kcal'].mean(),2)} \\n\"\n","      + f\"Mean value in Freshwater_100gProtein: {round(df_knn['Freshwater_100gProtein'].mean(),2)} \\n\"\n","      + f\"Mean value in Eutro_Em_100gProtein: {round(df_knn['Eutro_Em_100gProtein'].mean(),2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["Note that while the KNN imputation enjoys its fair share of popularity, this does not mean it is the optimal solution.\n","\n","KNN imputation still struggles with certain issues (like outlier values which is a big problem in this dataset) and may require some fine tuning to the \"k\" hyperparameter. Still, it can be an invaluable ally when dealing with missing values!"]},{"cell_type":"markdown","metadata":{},"source":["#### 5) MICE Imputation"]},{"cell_type":"markdown","metadata":{},"source":["For this section we will make use of the IterativeImputer library which features multivariate imputation.\n","This is because it utilizes multiple features  as opposed to the univariate imputation from SimpleImputer which deals with missing values using only data from one feature."]},{"cell_type":"markdown","metadata":{},"source":["Multiple Imputation by Chained Equations (MICE) is currently considered one of the most potent tools for dealing with missing data.\n","\n","The process involves multiple iteration circles. During each iteration selected missing data can be imputed through multiple predictive models. These range from standard mean imputation to linear regression and random forests.  Note that by default MICE makes use of BayesianRidge model but there exist specific options to instruct the algorithm which predictive method to use.\n","\n","Each missing value is treated as a depended variable and the remainder data is used to predict it. After each iteration circle imputed values from specific variables that resulted from that iteration's predictive model are used to impute missing data in other variables. The process is completed when all prediction modelling reaches its course by achieving an optimal result of convergence between existed data and missing data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.206312Z","iopub.status.busy":"2023-01-19T00:07:48.205832Z","iopub.status.idle":"2023-01-19T00:07:48.418464Z","shell.execute_reply":"2023-01-19T00:07:48.416981Z","shell.execute_reply.started":"2023-01-19T00:07:48.206273Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# MICE imputation\n","\n","# Retrieving numeric columns\n","df_numeric = df_food.select_dtypes('number') # a more explicit option to select numeric columns\n","df_mice = df_numeric.copy(deep=True)\n","\n","# Importing necessary libraries\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.linear_model import BayesianRidge\n","\n","# We will choose the default model used by the MICE algorithm: the Bayesian Ridge model.\n","mice_imputer = IterativeImputer(\n","    missing_values = np.nan,\n","    estimator = BayesianRidge(), \n","    initial_strategy = 'mean',   \n","    imputation_order = 'ascending',\n","    verbose = 1,\n","    max_iter = 9)\n","\n","df_mice = pd.DataFrame(mice_imputer.fit_transform(df_mice), columns = df_mice.columns)\n","df_mice.insert(0,\"Food_Product\", df_food[\"Food_Product\"])\n","df_mice.insert(1,\"Category\", df_food[\"Category\"])\n","\n","df_mice_na_sum = df_mice.isna().sum().sum()\n","\n","print(f\"\\nMethod: KNN Imputation \\n\" \n","      + f\"Number of missing values in dataset: {df_mice_na_sum} \\n\\n\"\n","      + f\"Mean value in Eutro_Em_1000kcal: {round(df_mice['Eutro_Em_1000kcal'].mean(),2)} \\n\"\n","      + f\"Mean value in Freshwater_100gProtein: {round(df_mice['Freshwater_100gProtein'].mean(),2)} \\n\"\n","      + f\"Mean value in Eutro_Em_100gProtein: {round(df_mice['Eutro_Em_100gProtein'].mean(),2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 6) Hot-Deck A: Custom-fitted imputation by average value"]},{"cell_type":"markdown","metadata":{},"source":["No imputation walkthrough would be complete without a custom-made imputation method created specifically that our unique dataset allowing us more freedom of engineering, albeit at the cost of a standardized methodology.\n","\n","For this hot-deck method we can try to blend in a few key features from our previous methods:\n","1. A useful perspective would be to cluster products based on their respective category where each food product would belong within a specific food category (already performed in the previous section)\n","2. Once all categories are set, we can utilize them to perform various imputation methods based on each product's respective category. We will use these newly created clusters to make accurate predictions about missing values by using several sub-segments of our dataset to predict similar values.\n","3. Finally, as an optional method we may blend in our previous imputation methodologies (e.g. like MICE) to further strengthen our algorithm."]},{"cell_type":"markdown","metadata":{},"source":["The [Category] column was created in the previous section. We can use this new column to further strengthen our imputation methodology and avoid certain imputation pitfalls from previous methods (e.g. standard mean imputation) that might affect our predicted values.\n","\n","For example, mean imputation will take the mean of all entries of [Food_Products] thus resuling in much higher or lower values for our missing values: the [Food_Product] entry entitled [Beef (beef herd)] scores the highest in almost every feature measuring CO2 emissions across the board while [Bananas] score considerably lower."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.421597Z","iopub.status.busy":"2023-01-19T00:07:48.421217Z","iopub.status.idle":"2023-01-19T00:07:48.452329Z","shell.execute_reply":"2023-01-19T00:07:48.451118Z","shell.execute_reply.started":"2023-01-19T00:07:48.421566Z"},"trusted":true},"outputs":[],"source":["# Looking at an example about product differences. In the two entries \"Beef\" and \"Bananas\" we can clearly see their massive difference across all features measuring CO2 emissions\n","\n","(df_food[df_food[\"Food_Product\"].str.contains(\"beef\") | (df_food[\"Food_Product\"] == \"Bananas\")])"]},{"cell_type":"markdown","metadata":{},"source":["So here's the key question: why would values originating from red meat and animal products be used to predict values of fruits or vegetables? If we are trying to predict missing values for low-CO2 emission food products from Vegetables and we bundle them with the high-CO2 emission from animal products wouldn't the newly imputed data values of vegetables be \"unnaturally\" higher?\n","\n","Let us rectify this issue. We will create several sub-datasets, each containing similar types of food [Categories]. These category-specific sub-datasets will be used to impute values for [Food_Products] contained within them. To put it more simply with an example: missing values in [Fruits] will use data from other [Fruits] or [Vegetables] for the imputation method.\n","\n","Since we already did the work of clustering each product in each [Category] let us make use of a simple mean imputation based on each slice of the newly formed datasets, and finally merge them all together again."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.45503Z","iopub.status.busy":"2023-01-19T00:07:48.454278Z","iopub.status.idle":"2023-01-19T00:07:48.508268Z","shell.execute_reply":"2023-01-19T00:07:48.506584Z","shell.execute_reply.started":"2023-01-19T00:07:48.454987Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Creating sub-datasets, creating a specialized \"imputation pool\" for clusters of food categories\n","df_veg_fruits = df_food[\n","    (df_food[\"Category\"] == \"Vegetables\") |\n","    (df_food[\"Category\"] == \"Fruits\")]\n","df_veg_fruits_mean = df_veg_fruits.fillna(df_veg_fruits.mean()) # performing a mean imputation for each missing value in our clusters\n","\n","# repeate the process for all other categories\n","df_grains_nuts = df_food[\n","    (df_food[\"Category\"] == \"Nuts\") |\n","    (df_food[\"Category\"] == \"Grains\")]\n","df_grains_nuts_mean = df_grains_nuts.fillna(df_grains_nuts.mean())\n","\n","df_anim_dairy = df_food[\n","    (df_food[\"Category\"] == \"Animal_Prod\") |\n","    (df_food[\"Category\"] == \"Dairy\")]\n","df_anim_dairy_mean = df_anim_dairy.fillna(df_anim_dairy.mean())\n","\n","df_oils_sugar_other = df_food[\n","    (df_food[\"Category\"] == \"Oils\") |\n","    (df_food[\"Category\"] == \"Sugar\") |\n","    (df_food[\"Category\"] == \"Other\")]\n","df_oils_sugar_other_mean = df_oils_sugar_other.fillna(df_oils_sugar_other.mean())\n","\n","# concatenating all newly created datasets holding specifically imputed values\n","df_sum = pd.concat([df_veg_fruits_mean,df_grains_nuts_mean,df_anim_dairy_mean,df_oils_sugar_other_mean]).round(2)\n","df_sum = df_sum.sort_index()\n","\n","print(f\"Method: Hot-Deck, Custom-Fitted Imputation \\n\" \n","      + f\"Number of missing values in dataset: {df_sum.isna().sum().sum()} \\n\\n\"\n","      + f\"Mean value in Eutro_Em_1000kcal: {round(df_sum['Eutro_Em_1000kcal'].mean(),2)} \\n\"\n","      + f\"Mean value in Freshwater_100gProtein: {round(df_sum['Freshwater_100gProtein'].mean(),2)} \\n\"\n","      + f\"Mean value in Eutro_Em_100gProtein: {round(df_sum['Eutro_Em_100gProtein'].mean(),2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 7) Hot-Deck B: custom-fitted imputation by MICE"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.510384Z","iopub.status.busy":"2023-01-19T00:07:48.509948Z","iopub.status.idle":"2023-01-19T00:07:48.689434Z","shell.execute_reply":"2023-01-19T00:07:48.687783Z","shell.execute_reply.started":"2023-01-19T00:07:48.510352Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Alternatively we can go one step further, utilizing both dataset-specific knowledge and methodology by adding MICE imputation method to our product segmentation\n","\n","# Sklearn Imputer Libraries\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.linear_model import BayesianRidge\n","\n","# Creating vegetables and fruits sub-dataset\n","df_veg_fruits = df_food[\n","    (df_food[\"Category\"] == \"Vegetables\") |\n","    (df_food[\"Category\"] == \"Fruits\")]\n","\n","# Retrieving the index from original sub-dataset (since the Imputer resets the index)\n","df_veg_fruits_index = df_veg_fruits.index\n","\n","# Retrieving only numeric columns\n","df_veg_fruits_num = df_veg_fruits.select_dtypes('number')\n","\n","# Imputer transformation\n","mice_imputer = IterativeImputer(\n","    missing_values = np.nan,\n","    estimator = BayesianRidge(),\n","    verbose = 0)\n","df_veg_fruits_num_mice = pd.DataFrame(mice_imputer.fit_transform(df_veg_fruits_num), columns = df_veg_fruits_num.columns)\n","\n","# Re-attaching index and numeric columns (as the first 2 columns)\n","df_veg_fruits_num_mice.set_index(df_veg_fruits_index, inplace = True)\n","df_veg_fruits_num_mice.insert(0, \"Food_Product\", df_veg_fruits[\"Food_Product\"])\n","df_veg_fruits_num_mice.insert(1, \"Category\", df_veg_fruits[\"Category\"])\n","\n","print(\"Number of missing values in df_veg_fruits: \" + str(df_veg_fruits_num_mice.isna().sum().sum()))"]},{"cell_type":"markdown","metadata":{},"source":["The process will repeat for all other sub-datasets which will not be explored since the process is similar to the one in HoT-Deck A approach, albeit with a higher level of complexity. This last approach is the most complicated one and does seem like overkill. Since time and computational requirements are ever-present factors we should opt for one of the previous methods. For this analysis we will make use of the MICE approach, however, the custom-fitted and KNN methods would also be acceptable."]},{"cell_type":"markdown","metadata":{},"source":["Our imputation section is complete.\n","\n","We have utilized seven different imputation methods, covering standard easy-to-implement models but also more advanced methodologies."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section-one\"></a>\n","# 5. Visualization and Analysis"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"subsection-five-one\"></a>\n","### 5.1 Setting up"]},{"cell_type":"markdown","metadata":{},"source":["As noted in the previous section, we will opt for the MICE imputation using Bayesian Ridge to deal with missing values which is the most elegant and potent option from aforementioned imputation methods."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.692055Z","iopub.status.busy":"2023-01-19T00:07:48.691486Z","iopub.status.idle":"2023-01-19T00:07:48.699921Z","shell.execute_reply":"2023-01-19T00:07:48.698137Z","shell.execute_reply.started":"2023-01-19T00:07:48.691986Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Choosing the MICE imputation as our default DataFrame\n","\n","df = df_mice\n","\n","print(df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.702654Z","iopub.status.busy":"2023-01-19T00:07:48.702075Z","iopub.status.idle":"2023-01-19T00:07:48.78744Z","shell.execute_reply":"2023-01-19T00:07:48.785865Z","shell.execute_reply.started":"2023-01-19T00:07:48.702605Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["**Operational Features:** There are some additional inconsistences in our dataset. The column [Total_Emissions] does not equate with the respective measurements in what we will call the \"Operational Features\": Land_Use_Change, Farm, Transport, Retail, Processing, Packaging and Animal Feed. After all, shouldn't a column entitled \"Total Emissions\" cover a grand sum of all emissions?\n","\n","After looking at the numeric values of each column it would appear that Retail emissions might actually be excluded from the [Total_Emissions] column. There is still a 0.3 value difference but at the very least we have noticed and partially solved another issue."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.790246Z","iopub.status.busy":"2023-01-19T00:07:48.789669Z","iopub.status.idle":"2023-01-19T00:07:48.803897Z","shell.execute_reply":"2023-01-19T00:07:48.802381Z","shell.execute_reply.started":"2023-01-19T00:07:48.790198Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["df_operations = df[['Land_Use_Change', 'Animal_Feed','Farm','Processing','Transport','Packaging','Retail']]\n","\n","print(f\"Sum of [Total_Emissions]: {df['Total_Emissions'].sum().sum()}\\n\" +\n","      f\"Sum of Operational Features: {df_operations.sum().sum().round()}\\n\" +\n","      f\"Sum without Retail: {df.iloc[:,2:8].sum().sum().round()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.80579Z","iopub.status.busy":"2023-01-19T00:07:48.805463Z","iopub.status.idle":"2023-01-19T00:07:48.844156Z","shell.execute_reply":"2023-01-19T00:07:48.842974Z","shell.execute_reply.started":"2023-01-19T00:07:48.805761Z"},"trusted":true},"outputs":[],"source":["# For starters, let's have a top-level picture:\n","\n","df_categ_group = df.groupby([\"Category\"]).sum().sort_values(by='Total_Emissions', ascending = False)\n","df_categ_group"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.845995Z","iopub.status.busy":"2023-01-19T00:07:48.845655Z","iopub.status.idle":"2023-01-19T00:07:48.901194Z","shell.execute_reply":"2023-01-19T00:07:48.900097Z","shell.execute_reply.started":"2023-01-19T00:07:48.845966Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Note that we can also utilize the pivot_table command for a similar result\n","cols_order = list(df.columns.values)\n","df_piv = df.round(2).pivot_table(cols_order,index='Category', aggfunc=\"sum\")\n","df_piv = df_piv.reindex(cols_order,axis=1) # re-indexing the columns in the same order as our dataset\n","df_piv.drop(df_piv.iloc[:,0:2],axis=1) # deleting the NaN columns"]},{"cell_type":"markdown","metadata":{},"source":["As expected, we can see that Animal Products are at the top of list for [Total_Emissions]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-19T00:07:48.903296Z","iopub.status.busy":"2023-01-19T00:07:48.902917Z","iopub.status.idle":"2023-01-19T00:07:52.14801Z","shell.execute_reply":"2023-01-19T00:07:52.145728Z","shell.execute_reply.started":"2023-01-19T00:07:48.903265Z"},"trusted":true},"outputs":[],"source":["# A quick overview of the primary \"operational\" features - we shall look into into those in greater detail.\n","\n","df_operational = df_food.iloc[:,1:9]\n","df_oper_col = df_operational.select_dtypes('number').columns\n","\n","for col in df_oper_col:\n","    fig, ax = plt.subplots(1, 3, figsize=(10,3))\n","    sns.histplot(data=df_operational, x=col, ax=ax[0]).set(title=\"Histogram\")\n","    sns.boxplot(data=df_operational, x=col, ax=ax[1]).set(title=\"Boxplot\")\n","    sns.violinplot(data=df_operational, x=col, ax=ax[2]).set(title=\"Violin Plot\")"]},{"cell_type":"markdown","metadata":{},"source":["We definetely have massive skewness issues and none even remotely resembles a normal distribution.\n","\n","Let's do a useful overview visualization showing the [Total_Emissions] by each [Food_Product] filtered by the food [Category]."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.149316Z","iopub.status.idle":"2023-01-19T00:07:52.149751Z","shell.execute_reply":"2023-01-19T00:07:52.149572Z","shell.execute_reply.started":"2023-01-19T00:07:52.149554Z"},"trusted":true},"outputs":[],"source":["sns.set_style(\"darkgrid\")\n","rel_plot = sns.relplot(data=df_food.sort_values(by='Category'), x='Total_Emissions', y='Food_Product',\n","            hue='Category', palette=\"bright\",\n","           height=7, aspect=1)\n","rel_plot.fig.suptitle(\"Total Emissions by Food Product, filtered by Food Category\")"]},{"cell_type":"markdown","metadata":{},"source":["It would appear that our clustering helped us get some much-needed insight. [Animal_Products] are (understandably) more erratic, while [Fruits], [Vegetables] and indeed most other food [Categories] are relatively stable as far as variability is concerned. The aptly named [Other] category that includes Coffee, Wine, Tofu and Dark Chocolate also holds a higher level of variability due to the fact that most of them are processed rather than raw products and, therefore, are expected to have greater fluctuations in measurements (in addition to being disimilar products). Note that the dataset does not mention Coffee Beans (which are actually derived from the pits of a fruit in its natural form) but rather Coffee which in most cases refers to the processed product."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"subsection-five-two\"></a>\n","### 5.2 Emissions by Product Category"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.151232Z","iopub.status.idle":"2023-01-19T00:07:52.151803Z","shell.execute_reply":"2023-01-19T00:07:52.151543Z","shell.execute_reply.started":"2023-01-19T00:07:52.151516Z"},"trusted":true},"outputs":[],"source":["pie_data = df.groupby(['Category'])[\"Total_Emissions\"].sum().sort_values()\n","labels = list(df[\"Category\"].unique())\n","explode_vals= [0.5,0.4,0.3,0.2,0.1,0,0,0,0]\n","color_vals = [\"khaki\", \"darkgreen\", \"darkorchid\", \"olive\", \"aqua\", \"grey\", \"gold\", \"green\", \"maroon\"]\n","\n","fig= plt.figure(figsize=(13,8), facecolor = '#eae4f2')\n","plt.pie(pie_data, labels=labels, autopct='%.1f%%', explode = explode_vals, colors = color_vals)\n","\n","donut = plt.Circle( (0,0), 0.7, color='white')\n","p = plt.gcf()\n","p.gca().add_artist(donut)\n","\n","plt.title('Percentage of Total Emissions, Segmentation by Food Category')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Animal Products appear to produce more [Total_Emissions] than the next three key sources (Fruits, Oil, and Other - processed products).\n","\n","Next, since we have 43 different [Food Products] we should look at obtaining a top 15 list."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.153733Z","iopub.status.idle":"2023-01-19T00:07:52.154489Z","shell.execute_reply":"2023-01-19T00:07:52.154274Z","shell.execute_reply.started":"2023-01-19T00:07:52.154243Z"},"trusted":true},"outputs":[],"source":["df_top15_emm = df.groupby(['Food_Product'])['Total_Emissions'].sum().sort_values(ascending=False).head(15)\n","diff_top15 = round(((df_top15_emm.sum()/df['Total_Emissions'].sum())*100), 1)\n","\n","labels_15 = df_top15_emm.index\n","explode_vals= [0,0,0,0,0,0,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n","\n","fig = plt.figure(figsize=(15,8),facecolor = '#eae4f2')\n","plt.pie(df_top15_emm, labels=labels_15, autopct='%.1f%%', explode=explode_vals)\n","\n","donut = plt.Circle((0,0), 0.7, color='white')\n","p = plt.gcf()\n","p.gca().add_artist(donut)\n","\n","plt.title('Total Emissions from the Top 15 Food Products')\n","\n","print(f\"Grand sum of Total_Emissions: {df['Total_Emissions'].sum()}\\n\" +\n","     f\"Sum of Top 15 Food_Products: {df_top15_emm.sum().round(1)}\\n\" +\n","     f\"Total Emission percentage of Top 15 Food Products: {diff_top15}%\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The Top 15 list of [Food_Products] amounts for approximately 85% of the total emissions from all 43 products present in our dataset. Since there are many proponents of the idea that pie charts should contain as little slices as possible we should zoom in closer and look at the Top 10 products just in case we can uncover a more snappy statistic for our hypothetical colleagues, customers and/or food and welfare organizations."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.156155Z","iopub.status.idle":"2023-01-19T00:07:52.156572Z","shell.execute_reply":"2023-01-19T00:07:52.156394Z","shell.execute_reply.started":"2023-01-19T00:07:52.156375Z"},"trusted":true},"outputs":[],"source":["df_top10_emm = df.groupby(['Food_Product'])['Total_Emissions'].sum().sort_values(ascending=False).head(10)\n","diff_top10 = round(((df_top10_emm.sum()/df['Total_Emissions'].sum())*100),1)\n","\n","labels = df_top10_emm.index\n","explode_vals= [0,0,0,0,0,0,0.1,0.15,0.2,0.25]\n","\n","fig = plt.figure(figsize=(13,8),facecolor = '#eae4f2')\n","plt.pie(df_top10_emm, labels=labels, autopct='%.1f%%', explode=explode_vals)\n","\n","donut = plt.Circle((0,0), 0.7, color='white')\n","p = plt.gcf()\n","p.gca().add_artist(donut)\n","\n","plt.title('Total Emissions from the Top 10 Food Products', fontsize=15)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.158396Z","iopub.status.idle":"2023-01-19T00:07:52.158815Z","shell.execute_reply":"2023-01-19T00:07:52.158638Z","shell.execute_reply.started":"2023-01-19T00:07:52.15862Z"},"trusted":true},"outputs":[],"source":["print(f\"Grand sum of Total_Emissions: {df['Total_Emissions'].sum()}\\n\" +\n","     f\"Sum of Top 10 Food_Products: {df_top10_emm.sum().round(1)}\\n\" +\n","     f\"Total Emission percentage of Top 10 Food Products: {diff_top10}%\")"]},{"cell_type":"markdown","metadata":{},"source":["This looks better. The Top 10 list of [Food_Products] amounts for approximately 75% or 3/4ths of the total emissions from all 43 products present in our dataset. This is an interesting discovery to help us narrow down the list of products and is a lot more... \"catchy\" if needed to communicate insights."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"subsection-five-three\"></a>\n","### 5.3 Agriculture Operational Emissions by Value Chain"]},{"cell_type":"markdown","metadata":{},"source":["We should keep looking at the operational features for more information. Let's visualize CO2 emissions in order of value chain from start to finish: Land-Use Change, Farming, Processing, Packaging, Transportation, and finally Retail.\n","\n","Let us showcase this visualization by keeping it simple and easily readable as dictated by the Zen of Python, and later on we will revisit this visualization and re-forge it using iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.159882Z","iopub.status.idle":"2023-01-19T00:07:52.160307Z","shell.execute_reply":"2023-01-19T00:07:52.160129Z","shell.execute_reply.started":"2023-01-19T00:07:52.160109Z"},"trusted":true},"outputs":[],"source":["# Visualizing CO2 emissions in order of product value chain without a loop\n","\n","data_land = df.groupby(['Category'])['Land_Use_Change'].sum().sort_values()\n","data_farm = df.groupby(['Category'])['Farm'].sum().sort_values()\n","data_proce = df.groupby(['Category'])['Processing'].sum().sort_values()\n","data_transp = df.groupby(['Category'])['Transport'].sum().sort_values()\n","data_pack = df.groupby(['Category'])['Packaging'].sum().sort_values()\n","data_ret = df.groupby(['Category'])['Retail'].sum().sort_values()\n","\n","labels = list(df['Category'].unique())\n","color_vals = [\"khaki\", \"darkgreen\", \"darkorchid\", \"olive\", \"aqua\", \"grey\", \"olive\", \"gold\", \"brown\"]\n","explode_vals= [0.6,0.5,0.4,0.3,0.2,0.1,0,0,0]\n","\n","fig, ((ax1,ax2), (ax3,ax4), (ax5, ax6)) = plt.subplots(3,2, figsize=(13,13),facecolor = '#eae4f2')\n","\n","ax1.pie(data_land, labels=labels, colors=color_vals, explode=explode_vals, autopct='%.1f%%')\n","ax1.set_title(\"Land-Use Change Emissions\")\n","ax2.pie(data_farm, labels=labels, colors=color_vals, explode=explode_vals, autopct='%.1f%%')\n","ax2.set_title(\"Farm Emissions\")\n","ax3.pie(data_proce, labels=labels, colors=color_vals, explode=explode_vals, autopct='%.1f%%')\n","ax3.set_title(\"Processing Emissions\")\n","ax4.pie(data_pack, labels=labels, colors=color_vals, explode=explode_vals, autopct='%.1f%%')\n","ax4.set_title(\"Packaging Emissions\")\n","ax5.pie(data_transp, labels=labels, colors=color_vals, explode=explode_vals, autopct='%.1f%%')\n","ax5.set_title(\"Transport Emissions\")\n","ax6.pie(data_ret, labels=labels, colors=color_vals, explode=explode_vals, autopct='%.1f%%')\n","ax6.set_title(\"Retail Emissions\")\n","\n","plt.suptitle('Greenhouse Emissions by Food Category',fontsize=16)"]},{"cell_type":"markdown","metadata":{},"source":["The greatest disparity appears in [Farm] emissions where [Animal Products] dominate with an astonishing 60.5% of emissions. [Packaging] and [Processing] emissions appear to be relatively more balanced, while [Transport] emissions are by far the most equal across the board (at least compared with other operations). Surprisingly, [Retail] emissions appear to concentrate around three major food categories: Oils, Fruits and Animal Products with a noticeable lack in most other categories."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"subsection-five-four\"></a>\n","### 5.4 Creating and Visualizing a Normalized Variable Index"]},{"cell_type":"markdown","metadata":{},"source":["**Why build a scientific index?** Due to the high number of features in our dataset it would be quite burdersome to visualize each and every one for both [Food_Products] and [Categories]. However, this is not a difficulty that stems from the part of the execution but rather from the point of view of the reader (i.e. our colleagues, science teams, engineering, management, etc.). Note that we have multiple columns measuring emissions based on kilocalories, kilograms and protein per 100 grams.\n","\n","AS scientists our job is condence information into a meaningful format for our colleagues.\n","\n","We could easily visualize all 20+ columns but then the readers and colleagues would have to scavenge for an insightful summary among dozens of graphics. \n","\n","Instead, let us create something more... digestible (pun intended) about our food product dataset: \n","1) create a new normalized feature\n","\n","2) capable of summarizing information\n","\n","3) constructed through multiple other features\n","\n","4) scaled as a min-max index between 0 and 10 with the standard formula\n","\n","(Note that \"index\" here refers to a scientific unit of measurement rather than a dataset index)"]},{"cell_type":"markdown","metadata":{},"source":["**Background Research:** After doing some background research I've ascertained that Eutrophication takes its name from the overabundance of nutrients in water and soil usually caused by overuse of man-made chemicals due to farming practices but also from natural processes without human intervention (the usual culprits usually being the overabundance of phosphorus and nitrogen in soil and waters). Eutrophication can affect plants and algae in soil, lakes, surfaces waters, small and large bodies of water and can be attributed to human farming practices in large agricultural facilities.\n"]},{"cell_type":"markdown","metadata":{},"source":["**Hypothesis 1A:** the amount of [Freshwater] and [Scarce-water] withdrawls is positively related to the amount of: a) [Farm] and [Land-Use Change] emissions (i.e. more crops and animal products require more land which in turn requires more water withdrawal); b) [Greenhouse] emissions (i.e. more products = more emissions; self-explanatory).\n","\n","**Hypothesis 1B:** Utilizing a measurement of 1000kcal (instead of 1kg or 100gr of protein) would be more effective to communicate insights across science, business operations, government institutions and NGOs since food products will be judged as a cost-benefit analysis between nutrition (and alleviation of world hunger) versus CO2 emissions. Additionally, it is expect that pure caloric measurements will be more effective in communicating useful solutions for world hunger initiatives rather than kilograms or grams of protein."]},{"cell_type":"markdown","metadata":{},"source":["**Limitations**: Please note that I have created this index simply for the purposes of examining this dataset - it is not a peer-reviewed formula. Specialized environmental scientists will be able to study emissions, chemical runoff due to eutriphication, land-use complications among other subjects in much greater depth than we can in this specific dataset.\n","\n","However, I believe it is important that we make an effort to simulate that process with the tools we have available. Although we have a sample with few entries it is still considered adequate. Running a Pearson r correlation satisfies (a) the level of measurement (our variables are continuous) and (b) we do have related pairs, however, c) we have a few issues from outliers (mostly from animal products) which we might not wish to drop (since they are an important part of dietary analysis)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.161795Z","iopub.status.idle":"2023-01-19T00:07:52.162975Z","shell.execute_reply":"2023-01-19T00:07:52.16276Z","shell.execute_reply.started":"2023-01-19T00:07:52.162725Z"},"trusted":true},"outputs":[],"source":["# Looking into the features that we will use for creation of a generalized index measurement formula\n","\n","df_corr_wel = df[['Eutro_Em_1000kcal', 'Freshwater_1000kcal', 'GGas_Em_1000kcal', 'ScarcWater_1000kcal', 'Land_Use_1000kcal', 'Land_Use_Change', 'Farm']]\n","\n","sns.heatmap(df_corr_wel.corr())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.164598Z","iopub.status.idle":"2023-01-19T00:07:52.164995Z","shell.execute_reply":"2023-01-19T00:07:52.164822Z","shell.execute_reply.started":"2023-01-19T00:07:52.164804Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# or a lot more readable for looking at specific features:\n","\n","df_corr_wel.corr().sort_values(by='Eutro_Em_1000kcal', ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["**Water-Land-Eutrophication (WEL) Index formula:** ((Freshwater Withdrawals per 1000kcals + Scarce Water Withdrawals per 1000kcals) * Eutrophication Emissions per 1000kcals) + Greenhouse Gas Emissions per 1000kcals / Land_Use_Change per 1000kcals + (Farm Emissions + Land-Use Change)/2\n","\n","**Min-max normalization formula:** (xi-np.min(x)) / (np.max(x)-np.min(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.166389Z","iopub.status.idle":"2023-01-19T00:07:52.166817Z","shell.execute_reply":"2023-01-19T00:07:52.166628Z","shell.execute_reply.started":"2023-01-19T00:07:52.166609Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Creating a new column with the WEL index\n","\n","df.insert(2, 'WEL_Index', (((df['Freshwater_1000kcal'] + df['ScarcWater_1000kcal']) * df['Eutro_Em_1000kcal']) + df['GGas_Em_1000kcal'] / (df['Land_Use_1000kcal'] + (df['Farm']+df['Land_Use_Change'])/2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.168128Z","iopub.status.idle":"2023-01-19T00:07:52.168541Z","shell.execute_reply":"2023-01-19T00:07:52.168355Z","shell.execute_reply.started":"2023-01-19T00:07:52.168337Z"},"trusted":true},"outputs":[],"source":["# normalization on a 1-10 scale based on min-max values \n","\n","df['WEL_Index'] = (df['WEL_Index']-np.min(df['WEL_Index']))/(np.max(df['WEL_Index'])-np.min(df['WEL_Index']))*10"]},{"cell_type":"markdown","metadata":{},"source":["Now let us create a couple of useful tree maps. Usually, they are not used to display extensive statistical insight, but rather to convey information with ease even to non-specialized readers."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.169878Z","iopub.status.idle":"2023-01-19T00:07:52.170351Z","shell.execute_reply":"2023-01-19T00:07:52.17015Z","shell.execute_reply.started":"2023-01-19T00:07:52.170131Z"},"trusted":true},"outputs":[],"source":["# Creating a general map tree with the WEL Index for food [Category]\n","\n","df_wel = pd.DataFrame(df.groupby(['Category'])['WEL_Index'].sum()).sort_values(by='WEL_Index')\n","\n","plt.rcParams['text.color'] = \"black\"\n","plt.rcParams['font.size'] = 13\n","colors = ['palegreen', 'maroon', 'olive', \"gold\", 'darkviolet', 'khaki', 'deepskyblue', 'forestgreen', 'brown']\n","fig = plt.figure(figsize=(12,9))\n","ax = fig.add_subplot()\n","\n","sqf.plot(sizes = df_wel['WEL_Index'],\n","         label = df_wel.index,\n","         alpha=1, pad=False,\n","         color = colors,\n","         ax=ax)\n","\n","plt.axis('off')\n","\n","plt.text(78, 100,\n","         'WEL Index, Product Category',\n","         fontsize = 20, \n","         color='grey', \n","         horizontalalignment='center',\n","         verticalalignment='bottom')\n","        \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.172636Z","iopub.status.idle":"2023-01-19T00:07:52.173029Z","shell.execute_reply":"2023-01-19T00:07:52.172856Z","shell.execute_reply.started":"2023-01-19T00:07:52.172838Z"},"trusted":true},"outputs":[],"source":["# Creating a map tree with percentages for the emissions of the Top 15 [Food_Products]\n","\n","df_fpr = pd.DataFrame(df.groupby(['Food_Product'])['WEL_Index'].sum()).sort_values(by='WEL_Index', ascending=False).head(15)\n","\n","# We will be adding percentage values in this tree map in order to increase its appeal to a wider audience\n","perc_vals = [f\"{i/df_fpr['WEL_Index'].sum()*100:.2f}%\" for i in df_fpr['WEL_Index']]\n","perc_vals = perc_vals[0:len(df_fpr)]\n","\n","labels = [f\"{i[0]}\\n{i[1]}\" for i in zip(df_fpr['WEL_Index'].index,perc_vals)]\n","\n","plt.rcParams['text.color'] = \"black\"\n","plt.rcParams['font.size'] = 10\n","colors = ['maroon', 'cyan', 'green', 'red', 'coral', 'gold', 'lawngreen', 'gray', 'azure', 'sienna', 'olive', 'orange', 'blue', 'aqua', 'lime']\n","fig = plt.figure(figsize=(12,9))\n","ax = fig.add_subplot()\n","\n","sqf.plot(sizes = df_fpr['WEL_Index'],\n","         label = labels,\n","         alpha=1, pad=False,\n","         color = colors,\n","        ax=ax)\n","\n","plt.axis('off')\n","\n","# We will add two titles with different fontsizes and different positioning in order to convey additional information.\n","# They need to be aligned precisely on the top right side of our tree map.\n","plt.text(72, 100,\n","         'WEL Index,\\nTop 15 Food Product Emissions\\n',\n","         fontsize = 22, \n","         color ='grey', \n","         horizontalalignment='center',\n","         verticalalignment='bottom')\n","\n","plt.text(69, 100,\n","         '\\nPercentage derived from grand total of food products',\n","         fontsize = 15, \n","         color ='grey', \n","         horizontalalignment='center',\n","         verticalalignment='bottom')\n","        \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.177364Z","iopub.status.idle":"2023-01-19T00:07:52.177824Z","shell.execute_reply":"2023-01-19T00:07:52.177638Z","shell.execute_reply.started":"2023-01-19T00:07:52.177619Z"},"trusted":true},"outputs":[],"source":["# Note: as mentioned earlier, Operational Emissions refers to CO2 emissions from the six features dealing with the Food Product lifecycle.\n","# These are: Land-Use Change, Farming, Processing, Packaging, Transportation, Retail\n","\n","y1 = df.groupby(['Category'])['Total_Emissions'].sum().sort_values(ascending=False)\n","x1 = y1.index\n","\n","y2 = df.groupby(['Food_Product'])['WEL_Index'].sum().sort_values(ascending=False).head(5)\n","x2 = y2.index\n","\n","y3 = df.groupby(['Food_Product'])['WEL_Index'].sum().sort_values(ascending=False).tail(5)\n","x3 = y3.index\n","\n","fig = plt.figure(figsize=(13,5))\n","\n","ax1 = fig.add_axes([0.1,0.2,0.95,0.95])\n","ax2 = fig.add_axes([0.7,0.78,0.6,0.3])\n","ax3 = fig.add_axes([0.7,0.35,0.6,0.3])\n","\n","# All food [Categories] are color-coded accordingly (e.g. all animal products are variations of red, vegetables variations of green, etc.)\n","colors1 = ['darkred', 'black','olive','darkturquoise','khaki','forestgreen','purple','gold','orange']\n","colors2 = ['darkred', 'darkred','forestgreen', 'darkred','darkred']\n","colors3 = ['forestgreen','gold','navy','olive','olive']\n","\n","ax1.bar(x=x1, height=y1, color=colors1)\n","ax1.spines.right.set_visible(False)\n","ax1.spines.top.set_visible(False)\n","ax1.set_title(\"Total Operational CO2 Emissions \\n(Color-Filtered by Product Category)\", alpha=1)\n","ax2.bar(x=x2, height=y2, alpha=0.85, color=colors2)\n","ax2.set_title(\"WEL Index CO2 Emissions: Top Five Products\", alpha=0.85)\n","ax3.bar(x=x3, height=y3, alpha=0.85, color=colors3)\n","ax3.set_title(\"WEL Index CO2 Emissions: Bottom Five Products\", alpha=0.85)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Finally, let us revisit the previous pie chart cluster visualization we generated earlier and re-create a grand overview, only this time we will be utilizing iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.179223Z","iopub.status.idle":"2023-01-19T00:07:52.179979Z","shell.execute_reply":"2023-01-19T00:07:52.17979Z","shell.execute_reply.started":"2023-01-19T00:07:52.179769Z"},"trusted":true},"outputs":[],"source":["data_emm = df.groupby(['Category'])['Total_Emissions'].sum().sort_values()\n","data_wel = df.groupby([\"Category\"])['WEL_Index'].sum().sort_values()\n","\n","# adding the previously created sub-dfs to a new summary df\n","df_sum = pd.concat([data_farm, data_proce, data_transp, data_pack, data_transp, data_ret, data_emm, data_wel], axis=1)\n","\n","df_sum_t = df_sum.T.round(2) # Transpose the grouped dataframe and round to 2 decimal points"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.181707Z","iopub.status.idle":"2023-01-19T00:07:52.182136Z","shell.execute_reply":"2023-01-19T00:07:52.181932Z","shell.execute_reply.started":"2023-01-19T00:07:52.181912Z"},"trusted":true},"outputs":[],"source":["# In the following section the part: ax = axes[row_number // 3, row_number % 3] is a crucial cog in the iteration and quite necessary to the position of each chart.\n","# If I may assist you, the reader, understand its function better consider the following simple loop:\n","\n","for i in range(9):\n","    print(i//3,i%3)\n","\n","# The output reveals the row,col position of each chart - hopefully this will ease the understanding of the following iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:52.183262Z","iopub.status.idle":"2023-01-19T00:07:52.18366Z","shell.execute_reply":"2023-01-19T00:07:52.183486Z","shell.execute_reply.started":"2023-01-19T00:07:52.183468Z"},"trusted":true},"outputs":[],"source":["font_color = '#333c43'\n","color_vals = [\"purple\", \"gold\", \"brown\", \"darkgreen\", \"khaki\", \"olive\", \"aqua\", \"grey\", \"maroon\"]\n","\n","fig, axes = plt.subplots(3, 3, figsize=(13, 10), facecolor='#e8f4f0')\n","fig.delaxes(ax = axes[2,2]) # delete the unneeded blank axes from the 3x3 plt.subplots\n","\n","# iterating through the dataframe \n","for row_number, (idx, row) in enumerate(df_sum_t.iterrows()):\n","    ax = axes[row_number // 3, row_number % 3]\n","    ax.pie(row, \n","           labels=row.values, \n","           startangle=30,\n","           colors=color_vals, \n","           textprops={'color':font_color})\n","    ax.set_title(idx, fontsize=16, color=font_color)\n","    \n","    legend = plt.legend([x for x in row.index], \n","                        bbox_to_anchor=(1.8, .87), # modifying position of legend\n","                        loc='upper left',  \n","                        ncol=1)\n","    for text in legend.get_texts():\n","        plt.setp(text, color=font_color) # changing legend color\n","\n","fig.subplots_adjust(wspace=.01) # adjusting space between charts\n","\n","title = fig.suptitle('Total Emissions by Operational Features and WEL Index', y=.95, fontsize=20, color=font_color)\n","\n","plt.subplots_adjust(top=0.85, bottom=0.15)"]},{"cell_type":"markdown","metadata":{},"source":["Our analysis is complete. We addressed key issues like negative values and missing values, performed various imputation methods, compressed certain choosen features to generate a new overaching index variable, and performed various visualizations aimed at communicating different story-telling insights ranging from food product-specific, to top-tier and low-tier product emissions, easy-to-digest tree maps for quick communication to various stakeholders, as well as an extensive overview of operational emissions across the value chain.\n","\n","I hope my description was useful to you, the reader, across various sections of the methodology and maybe you picked up a code section or two to mold and adapt to your own analyses!\n","\n","All the best."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"vscode":{"interpreter":{"hash":"5ffd7eb2cebf9ac436b5021ba01877e9cee6b03524e01bf8c8637d3e64111215"}}},"nbformat":4,"nbformat_minor":4}
